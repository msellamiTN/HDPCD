1) To SCP into hortonworks sandbox use the below command
	scp file-name root@192.168.28.129:/root/data
    
2) To create a folder under /user directory :
	sudo -u hdfs hadoop fs -mkdir /user/root
    
3) To change the owner of the new directory to root :
	sudo -u hdfs hadoop fs -chown root:hdfs /user/root

4) To copy a file from local file system to HDFS:
	* First Create A File using : touch testing1.txt (under root or any directory)
	* Edit it using : vi testing1.txt
	* Use Command : hadoop fs -copyFromLocal /root/testing1.txt /user/root
	* Use Command : hadoop fs -put /root/testing1.txt /user/root ( alternative )
	* To Verify : hadoop fs -ls /user/root ( list all files under /user/root)
	* To display contents of a file : hadoop fs -cat /user/root/testing1.txt
	* To connect to a different IP address use : hadoop fs -ls hdfs://192.168.28.129/user/root (hdfs://{ip-address}/{directory})

5) mkdir
	* To create a user space in HDFS use mkdir
	* Use -p to recursively create directories : hadoop fs -mkdir /user/root/dir1/dir2/dir3
	* To Verify use : hadoop fs -ls -R /user/root
	* Use Patterns in mkdir : hadoop fs -mkdir /user/root/t{1,2} (t{1..10})
	* Use Ip address : hadoop fs -mkdir hdfs://192.168.28.129/user/root/a1 hdfs://192.168.28.129/user/root/a2

6) sqoop create directory for practice :
	* hadoop fs -mkdir /user/sqoop
	* sudo -u hdfs hadoop fs -chown root:hdfs /user/sqoop
	* sudo -u hdfs hadoop fs -chmod 777 /user/sqoop
	* hadoop fs -mkdir /user/sqoop/practice1
	* sudo -u hdfs hadoop fs -chown root:hdfs /user/sqoop/practice1
	* sudo -u hdfs hadoop fs -chmod 777 /user/sqoop/practice1 
	
7) sqoop list-databases command :

	sqoop list-databases \
	—-connect “jdbc:mysql://192.168.28.129:3306” \
	—-username retail_dba
	—-password hadoop

8) sqoop list-tables command :
	
	sqoop list-tables \
	—-connect “jdbc:mysql://192.168.28.129:3306/retail_db” \
	—-username retail_dba \
	—-password hadoop

9) sqoop eval command :

	sqoop eval \
	—-connect “jdbc:mysql://sandbox.hortonworks.com:3306/retail_db” \
	—-username retail_dba \
	—-password hadoop \
	—-query “SELECT * FROM departments”

10) sqoop import all tables :
	
	* Create directory for storing all the tables -> hadoop fs -mkdir /user/sqoop/practice1/all_tables
	* sudo -u hdfs hadoop fs -chown root:hdfs /user/sqoop/practice1/all_tables
	* sudo -u hdfs hadoop fs -chmod 777 /user/sqoop/practice1/all_tables
	* sqoop import-all-tables \
	  --connect jdbc:mysql://sandbox.hortonworks.com:3306/retail_db \
	  --username retail_dba \
	  --password haddop \
	  -m 1 \
	  --warehouse-dir /user/sqoop/practice1/all_tables

11) sqoop import-all-tables into hive :
	* Launch hive -> hive
	* Create Database -> create database sqoop_import
	* sqoop import-all-tables \
	 --connect jdbc:mysql://sandbox.hortonworks.com:3306/retail_db \
	 --username retail_dba \
	 --password hadoop \
	 --hive-import \
	 --hive-overwrite \
	 --create-hive-table \
	 --hive-database sqoop_import \
	 --compress \
	 --compression-codec org.apache.hadoop.io.compress.SnappyCodec

12) sqoop simple import :
	
	* Create target Directory : hadoop fs -mkdir /user/root/departments
	* Change Owner : sudo -u hdfs hadoop fs -chown root:hdfs /user/root/departments
	* Change Permissions : sudo -u hdfs hadoop fs -chmod 777 /user/root/departments
	* sqoop import \	
	  —-connect jdbc:mysql://sandbox.hortonworks.com:3306/retail_db \
	  —-username retail_dba \
	  —-password hadoop \
	  —-table departments \
	  —-target-dir /user/root/departments 

13) sqoop boundary-query and Columns:
	* To Remove a file : hadoop fs -rm -R /user/root/departments
	* sqoop import \	
	  —-connect jdbc:mysql://sandbox.hortonworks.com:3306/retail_db \
	  —-username retail_dba \
	  —-password hadoop \
	  —-table departments \
	  —-target-dir /user/root/departments \
	  -m 2 \
	  --boundary-query "select min(department_id), max(department_id) from departments where department_id <> 8000" \
          --columns department_name

14) sqoop split-by column_name when there is no primary key on a table :
	
	  sqoop import \	
	  —-connect jdbc:mysql://sandbox.hortonworks.com:3306/retail_db \
	  —-username retail_dba \
	  —-password hadoop \
	  —-table departments_nopk \
	  —-target-dir /user/root/departments_nopk \
	  —split-by department_id

15) sqoop —query argument demo :

sqoop import \	
—-connect jdbc:mysql://sandbox.hortonworks.com:3306/retail_db \
—-username retail_dba \
—-password hadoop \
—query “select * from orders join order_items on orders.order_id = order_items.order_item_order_id where \$CONDITIONS” \
—-table departments_nopk \
—-target-dir /user/root/orders_join \
—split-by order_id


16) sqoop where argument demo :

sqoop import \	
	  —-connect jdbc:mysql://sandbox.hortonworks.com:3306/retail_db \
	  —-username retail_dba \
	  —-password hadoop \
	  —-table departments \
	  —-target-dir /user/root/departments \
          —append \
	  -m 2 \
	  —where "department_id > 8" \
          --columns department_name
	
17) sqoop import into hive table already created :
	
* Create a database and table in hive for this demo:
		-> create database sqoop_import_hive;
		-> create table departments(department_id int, department_name string);
* sqoop import \
—-connect jdbc:mysql://sandbox.hortonworks.com:3306/retail_db \
—-username retail_dba \
—-password hadoop \
—-table departments \
—-hive-home /apps/hive/warehouse \  
—-hive-import \
—-hive-overwrite \
—-hive-table sqoop_import_hive.departments  

18) sqoop import into hive table and create the table while importing :

sqoop import \
—-connect jdbc:mysql://sandbox.hortonworks.com:3306/retail_db \
—-username retail_dba \
—-password hadoop \
—-table categories \
—-hive-home /apps/hive/warehouse \  
—-hive-import \
—-hive-overwrite \
—-hive-table sqoop_import_hive.categories \
—-create-hive—table 


19) sqoop import into hive incremental :

sqoop import \
--connect jdbc:mysql://sandbox.hortonworks.com:3306/retail_db \ 
--username retail_dba \
—-password hadoop \
—-table departments \
—-where "department_id < 6" \
—-target-dir /user/root/incremental_load


sqoop import \
--connect jdbc:mysql://sandbox.hortonworks.com:3306/retail_db \ 
--username retail_dba \
—-password hadoop \
—-table departments \
—-target-dir /user/root/incremental_load \
—-append \
—-check-column “department_id” \
—-incremental append \
—-last-value 5

20) sqoop export simple example :

create table order_items_export as select * from order_items where 1 = 2;

sqoop export \
--connect jdbc:mysql://sandbox.hortonworks.com:3306/retail_db \ 
--username retail_dba \
—-password hadoop \
—-table order_items_export \
—-export-dir /user/sqoop/practice1/all_tables/order_items \
—-batch 

21) sqoop export merge or update:

vi dep_export.txt
7, fanshop
9000, testing export

hadoop fs -mkdir /user/sqoop/export_test
hadoop fs -put dep_export.txt /user/sqoop/export_test

sqoop export \
--connect jdbc:mysql://sandbox.hortonworks.com:3306/retail_db \ 
--username retail_dba \
—-password hadoop \
—-table departments \
—-export-dir /user/sqoop/export_test \
—-batch
—-update-key department_id \
—-update-mode allowinsert

If the target table does not have any primary/unique key the data will be just duplicated.

22) sqoop import delimiter enclosed-by, fields-terminated-by lines-terminated-by -> plain hfs import

sqoop import
--connect jdbc:mysql://sandbox.hortonworks.com:3306/retail_db \ 
--username retail_dba \
—-password hadoop \
—-table departments \
—-target-dir /user/sqoop/partice1/departments_enclosed \
—-enclosed-by \” \
—-fields-terminated-by \| \
—-lines-terminated-by \:

23) null-string and null-non-string

MySQL ->

create table departments_null_nonnull(department_id int, department_name varchar(100));
insert into departments_null_nonnull select * from departments;
insert into departments_null_nonnull values(null, null);

sqoop import
--connect jdbc:mysql://sandbox.hortonworks.com:3306/retail_db \ 
--username retail_dba \
—-password hadoop \
—-table departments_null_nonnull \
—-hive-import \
—-hive-home /apps/hive/warehouse \
—-hive-table sqoop_import_hive.departments_null_nonnull \
—-create-hive-table \
-m -4 \
—-null-string “NULL” \
—-null-non-string -1 \
—-split-by department_id

24) sqoop export delimiters : 

create table departments_null_nonnull_export 
select * from departments_null_nonnull where 1 = 2;


sqoop export \
--connect jdbc:mysql://sandbox.hortonworks.com:3306/retail_db \ 
--username retail_dba \
—-password hadoop \
—-table departments_null_nonnull_export \
—-export-dir /apps/hive/warehouse/sqoop_import_hive.db/departments_null_nonnull \
—-input-fields-terminated-by ‘\001’ \
—-input-lines-terminated-by ‘\n’ \
—-input-null-string lvl \
—-input-non-null-string -1

————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————

Sqoop File Format -> —-as-textfile is default

25) —-as-sequencefile ->

sqoop import \
—-connect jdbc:mysql://sandbox.hortonworks.com:3306/retail_db \
—-username retail_dba \
—-password hadoop \
—-table departments \
—-as-sequencefile \
—-target-dir /user/sqoop/practice1/departments_sequencefile

26) —-as-avrodatafile ->
sqoop import \
—-connect jdbc:mysql://sandbox.hortonworks.com:3306/retail_db \
—-username retail_dba \
—-password hadoop \
—-table departments \
—-as-avrodatafile \
—-target-dir /user/sqoop/practice1/dep_avro_data_file

A file with the {table_name}.avsc extension will be created in the directory from where the sqoop import command was run.

cat {table_name}.avsc

To create a table in hive store this metadata in HDFS and provide the location of this metadata to the create table statement in hive

hadoop fs -mkdir -p /user/sqoop/avro_metadata
hadoop fs -put departments.avsc /user/sqoop/avro_metadata

CREATE EXTERNAL TABLE departments_avro
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
LOCATION 'hdfs:///user/sqoop/practice1/dep_avro_data_file'
TBLPROPERTIES ('avro.schema.url'='hdfs://sandbox.hortonworks.com/user/sqoop/avro_metadata/departments.avsc')

--Create Hive Table by including --driver argument :
sqoop import \
--connect jdbc:mysql://localhost:3306/retail_db \
--username retail_dba \
--password hadoop \
--table orders \
--hive-import \
--create-hive-table \
--hive-table pig_demo.orders \
--driver com.mysql.jdbc.Driver

Resolution for Sqoop with Avro
sqoop import-all-tables -Dmapreduce.job.user.classpath.first=true \
-m 12 \
--connect jdbc:mysql://localhost:3306/retail_db \
--username retail_dba \
--password hadoop \
--as-avrodatafile \
--warehouse-dir /apps/hive/warehouse/retail_stage.db
