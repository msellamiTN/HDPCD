1) Create student.txt input file using :
 vi student.txt
John,18,4.0
Mary,19,3.8
Bill,20,3.9
Joe,18,3.8

2) Push it to hdfs :
hadoop fs -put student.txt /user/pig_demo

3)Create a relation with Schema:
A = LOAD '/user/pig_demo/student.txt' USING PigStorage(',') AS (name:chararray, age:int, gpa:float); 

4) To verify the schema :
DESCRIBE A;

5) To display result on the screen :
DUMP A;

6) TO define a Tuple in Schema :
	*Create data4.txt input file using :
		(3,8,9)
		(1,4,7)
		(2,5,8)
	* Load it to hdfs using :
	  hadoop fs -put data4.txt /user/pig_demo/data4.txt
	*Define Relation using :
	 A = LOAD ‘/user/pig_demo/data4.txt’ AS (T: tuple(f1:int, f2:int, f3:int));
	 DESCRIBE A;
	 DUMP A;

7) To define a Bag in Schema :

* Create data_bag.txt input file using:
	{(3,8,9)}
	{(1,4,7)}
	{(2,5,8)}
* Load it to hdfs using :
  hadoop fs -put data_bag.txt /user/pig_demo

* Define a Relation using :

  B = LOAD ‘/user/pig_demo/data_bag.txt’ AS (B: bag{T: tuple(f1:int,f2:int,f3:int)});
  DESCRIBE B;
  DUMP B;

8) To define a Map in Schema :
* Create data_map input file using:
  vi data_map.txt
	[name#bob]
	[age#55]
* Load it into hdfs using :
  hadoop fs -put data_map.txt /user/pig_demo

* Define a Relation using :
  
  C = LOAD ‘/user/pig_demo/data_map.txt’ AS (M: map []);
  DESCRIBE C;
  DUMP C;

9) Operators Demo :

Modulo:

* Create data_add_operation.txt using:
vi data_add_operation.txt
10,1,{(2,3),(4,6)}
10,3,{(2,3),(4,6)}
10,6,{(2,3),(4,6),(5,7)}

* Load it into hdfs using :
hadoop fs -put data_add_operation.txt /user/pig_demo

* Define a Relation using :

  A = LOAD ‘/user/pig_demo/data_add_operation.txt’ AS (f1:int, f2:int, B:bag{T:tuple(t1:int,t2:int)});
  X = FOREACH A GENERATE f1,f2, f1%f2;
  DUMP X;

* BINCOUNT Example :
  X = FOREACH A GENERATE f2, (f2==1?1:COUNT(B));
  DUMP X;

* Boolean Operation:
  vi data_boolean_operation.txt
  1       2       3
  4       2       1
  8       3       4
  4       3       3
  7       2       5
  8       4       3
  hadoop fs -put data_boolean_operation.txt
  A = LOAD '/user/pig_demo/data_boolean_operation.txt' AS (f1:int,f2:int,f3:int);
  X = FILTER A BY (f1==8) OR (NOT (f2+f3>f1));

10) Casting Demo :
hadoop fs -put data_boolean_operation.txt /user/pig_demo/data_cast.txt
A = LOAD '/user/pig_demo/data_cast.txt' AS (f1:int,f2:int,f3:int);
B = GROUP A BY f1;
X = FOREACH B GENERATE group, (chararray)COUNT(A) AS Total;
DESCRIBE X;
X: {group: int,Total: chararray} -> Int casted to chararray

11) ByteArray Casting Demo :
vi data_bytearray_cast.txt
(1,2,3)
(4,2,1)
(8,3,4)
hadoop fs -put data_bytearray_cast.txt /user/pig_demo
A = LOAD '/user/pig_demo/data_bytearray_cast.txt' AS fld:bytearray;
DUMP A;
DESCRIBE A; -> A: {fld: bytearray}
X = FOREACH A GENERATE (tuple(int,int,float))fld;
DESCRIBE X; -> X: {fld: (int,int,float)}

12) Comparison Operators :

vi data_comparision.txt
11      apache
12      hadoop
13      google
14      bigdata
15      java
16      hdfs
17      pig
18      hive
19      mapreduce

hadoop fs -put data_comparision.txt /user/pig_demo


A = LOAD '/user/pig_demo/data_comparision.txt' AS (f1:int,f2:chararray);

X = FILTER A BY (f1 == 18);
DUMP X; -> (18,hive)

X = FILTER A BY (f2 == 'hadoop');
DUMP X; -> (12,hadoop)

X = FILTER A BY (f2 matches 'hdfs');
DUMP X; -> (16,hdfs)

13) Relational Operators :

* Cross :->
  vi data_relation_one.txt
  1       2       3
  4       2       1  

  vi data_relation_two.txt
  2       4
  8       9
  1       3

  hadoop fs -put data_relation_one.txt data_relation_two.txt /user/pig_demo

  A = LOAD '/user/pig_demo/data_relation_one.txt' AS (a1:int,a2:int,a3:int);
  B = LOAD ‘/user/pig_demo/data_relation_two.txt’ AS (a1:int,a2:int);
  X = CROSS A,B;

* Distinct :->
  vi data_distinct.txt
  8       3       4
  1       2       3
  4       3       3
  4       3       3
  1       2       3

  hadoop fs -put data_distict.txt /user/pig_demo
  A = LOAD '/user/pig_demo/data_distinct.txt' AS (a1:int,a2:int,a3:int);
  X = DISTINCT A;

* Filter :->
  
  vi data_filter.txt
  1       2       3
  4       2       1
  8       4       3
  4       3       3
  7       2       5

  hadoop fs -put data_filter.txt /user/pig_demo

  A = LOAD '/user/pig_demo/data_filter.txt' AS (a1:int,a2:int,a3:int);
  X = FILTER A BY a3 == 3;

* FOREACH :->

  vi data_foreach.txt
  1       2       3
  4       2       1
  8       4       3
  4       3       3
  7       2       5
  A = LOAD '/user/pig_demo/data_foreach.txt' AS (a1:int,a2:int,a3:int);
  X = FOREACH A GENERATE a1;
  DUMP X;

* Group :->

  vi data_group.txt ( Tab Delimited)
  John	18 	4.0
  Mary	19	3.8
  Bill	20 	3.9
  Joe	18	3.8
  A = LOAD '/user/pig_demo/data_group.txt' AS (name:chararray, age:int,gpa:float);
  DUMP A;
  B = GROUP A BY age;
  (18,{(Joe,18,3.8),(John,18,4.0)})
  (19,{(Mary,19,3.8)})
  (20,{(Bill,20,3.9)})

* Join (inner):
  vi data_join_inner_one.txt
  1       2       3
  4       2       1
  8       3       4
  4       3       3
  7       2       5
  8       4       3

  vi data_join_inner_two.txt
  2       4
  8       9
  1       3
  2       7
  2       9
  4       6
  4       9
 
  A = LOAD '/user/pig_demo/data_join_inner_one.txt' AS (a1:int,a2:int,a3:int);
  B = LOAD '/user/pig_demo/data_join_inner_two.txt' AS (b1:int,b2:int);
  X = JOIN A BY a1, B BY b1;
  DUMP X;

* Join (outer):
  vi data_join_outer_one.txt
  alice   101
  bob     102
  hary    103
  molly   104
  mark    105
  natsy   106
  mikasa  107
  pikachu 108
  doremon 109
  raichu  110

  vi data_join_inner_two.txt
  alice   Hadoop
  bob     hdfs    
  harry   sqoop
  molly   hive
  mark    pig
  viraku  spark
  samuel  spork
  sunny   llama
  winskey kafka

  A = LOAD '/user/pig_demo/data_join_outer_one.txt' AS (name:chararray, id:int);
  B = LOAD '/user/pig_demo/data_join_outer_two.txt' AS (name:chararray, tools:chararray);

  LEFT OUTER JOIN :->
  C = JOIN A BY $0 LEFT OUTER, B BY $0;
  DUMP C;
  (bob,102,bob,hdfs)
  (hary,103,,)
  (mark,105,mark,pig)
  (alice,101,alice,Hadoop)
  (molly,104,molly,hive)
  (natsy,106,,)
  (mikasa,107,,)
  (raichu,110,,)
  (doremon,109,,)
  (pikachu,108,,)

  RIGHT OUTER JOIN :->
  C = JOIN A BY $0 RIGHT OUTER, B BY $0;
  (bob,102,bob,hdfs)
  (mark,105,mark,pig)
  (alice,101,alice,Hadoop)
  (,,harry,sqoop)
  (molly,104,molly,hive)
  (,,sunny,llama)
  (,,samuel,spork)
  (,,viraku,spark)
  (,,winskey,kafka)

  FULL OUTER JOIN :->
  C = JOIN A BY $0 FULL, B BY $0;
  DUMP C;
  (bob,102,bob,hdfs)
  (hary,103,,)
  (mark,105,mark,pig)
  (alice,101,alice,Hadoop)
  (,,harry,sqoop)
  (molly,104,molly,hive)
  (natsy,106,,)
  (,,sunny,llama)
  (mikasa,107,,)
  (raichu,110,,)
  (,,samuel,spork)
  (,,viraku,spark)
  (doremon,109,,)
  (pikachu,108,,)
  (,,winskey,kafka)

* LIMIT :
  A = LOAD '/user/pig_demo/data_join_inner_one.txt' AS (a1:int,a2:int,a3:int);
  LIMIT A 3;

* Order By :
  A = LOAD '/user/pig_demo/data_orderby.txt' AS (a1:int,a2:int,a3:int);
  X = ORDER A BY a3 DESC;

* Rank :
  vi data_rank.txt
  David   1       N
  Tete    2       N
  Ranjit  3       M
  Ranjit  3       P
  David   4       Q
  David   4       Q
  Jillian 5       Q
  
  hadoop fs -put data_rank.txt /user/pig_demo
  A = LOAD '/user/pig_demo/data_rank.txt' as (f1:chararray,f2:int,f3:chararray);
  DUMP A;
  B = RANK A;

* SPLIT :
  vi data_split.txt
  1       2       3
  4       5       6
  7       8       9

  SPLIT A INTO X IF f1 < 7, Y IF f2 == 5, Z IF (f3 < 6 OR f3 > 6);
  DUMP X;
  DUMP Y;
  DUMP Z;

* Store :
  A = LOAD '/user/pig_demo/data_orderby.txt' AS (a1:int,a2:int,a3:int);
  STORE A INTO ’/user/pig_demo/MyStoreOutput’ USING PigStorage(‘*’);
  cat /user/pig_demo/MyStoreOutput;
  1*2*3
  4*2*1
  8*3*4
  4*3*3
  7*2*5
  8*4*3

* Union :
  vi data_union_1.txt
  1       2       3
  4       2       1
  vi data_union_2.txt
  2       4
  8       9
  1       3
  A = LOAD '/user/pig_demo/data_union_one.txt' AS (a1:int,a2:int,a3:int);
  B = LOAD '/user/pig_demo/data_union_two.txt' AS (b1:int,b2:int);
  X = UNION A,B;


Create NYSE_DAILY Table in Hive :->

create table nyse_daily(
    > nexchange string,
    > symbol string,
    > DDate date,
    > open double,
    > high double,
    > low double,
    > close double,
    > volume int,
    > adj_close double)
    > ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';

pig -useHCatalog
nyse_daily = LOAD 'nyse.nyse_daily' USING 'org.apache.hive.hcatalog.pig.HCatLoader()';
describe nyse_daily;
grp = group nyse_daily by symbol;
symbol_count = FOREACH grp GENERATE group, COUNT_STAR(nyse_daily.symbol);
DUMP symbol_count;

Joins and Group By Demos :
collisions = load 'NYPD_Motor_Vehicle_Collisions.csv' using PigStorage(',');
collisions_limited = limit collisions 100000;

-- To Extract Header Information
collisions_header = limit collisions_limited 1;

-- Relation with useful data
collisions_useful = foreach collisions_limited generate $0 as date, $2 as borough, $3 as zipcode, TRIM($8) as location, $11 + $13 + $15 + $17 as injured, TRIM($19) as reason;

collisions_reason_injured = foreach collisions_useful generate reason, borough, injured;

-- Group By Reason
collisions_reason_group = group collisions_reason_injured by reason;
describe collisions_reason_group;
     collisions_reason_group: {group: chararray,collisions_reason_injured: {(reason: chararray,borough: bytearray,injured: double)}
    = group collisions_reason_injured by borough;

total_injured_by_reason = foreach collisions_reason_group generate group, SUM(collisions_reason_injured.injured); 
num_collisions_by_borough = foreach collisions_borough_group generate group, COUNT(collisions_reason_injured.injured);

-- Joins
names = load 'names.csv' using PigStorage(',') as (symbol:chararray, name:chararray, revenue:chararray);
trades = load 'trades.csv' using PigStorage(',') as (symbol: chararray, open: double, high: double, low: double, close: double, date: datetime); 
names_trades = join names by symbol, trades by symbol;
names_trades_useful = foreach names_trades generate names::symbol, revenue, close, date;
